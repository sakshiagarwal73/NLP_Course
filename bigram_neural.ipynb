{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigram_neural.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6AkujF7XpiY/R3DuF2JPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonakshisen1234/NLP_Course/blob/master/bigram_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4qfG08Expb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import operator\n",
        "import numpy as np\n",
        "from future.utils import iteritems\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "KEEP_WORDS = set([\n",
        "  'king', 'man', 'queen', 'woman',\n",
        "  'italy', 'rome', 'france', 'paris',\n",
        "  'london', 'britain', 'england',\n",
        "])\n",
        "\n",
        "def get_sentences():\n",
        "  # returns 57340 of the Brown corpus\n",
        "  # each sentence is represented as a list of individual string tokens\n",
        "  return brown.sents()\n",
        "\n",
        "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
        "  sentences = get_sentences()\n",
        "  indexed_sentences = []\n",
        "\n",
        "  i = 2\n",
        "  word2idx = {'START': 0, 'END': 1}\n",
        "  idx2word = ['START', 'END']\n",
        "\n",
        "  word_idx_count = {\n",
        "    0: float('inf'),\n",
        "    1: float('inf'),\n",
        "  }\n",
        "\n",
        "  for sentence in sentences:\n",
        "    indexed_sentence = []\n",
        "    for token in sentence:\n",
        "      token = token.lower()\n",
        "      if token not in word2idx:\n",
        "        idx2word.append(token)\n",
        "        word2idx[token] = i\n",
        "        i += 1\n",
        "\n",
        "      # keep track of counts for later sorting\n",
        "      idx = word2idx[token]\n",
        "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
        "\n",
        "      indexed_sentence.append(idx)\n",
        "    indexed_sentences.append(indexed_sentence)\n",
        "\n",
        "\n",
        "\n",
        "  # restrict vocab size\n",
        "\n",
        "  # set all the words I want to keep to infinity\n",
        "  # so that they are included when I pick the most\n",
        "  # common words\n",
        "  for word in keep_words:\n",
        "    word_idx_count[word2idx[word]] = float('inf')\n",
        "\n",
        "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  word2idx_small = {}\n",
        "  new_idx = 0\n",
        "  idx_new_idx_map = {}\n",
        "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
        "    word = idx2word[idx]\n",
        "    #print(word, count)\n",
        "    word2idx_small[word] = new_idx\n",
        "    idx_new_idx_map[idx] = new_idx\n",
        "    new_idx += 1\n",
        "  # let 'unknown' be the last token\n",
        "  word2idx_small['UNKNOWN'] = new_idx \n",
        "  unknown = new_idx\n",
        "\n",
        "  assert('START' in word2idx_small)\n",
        "  assert('END' in word2idx_small)\n",
        "  for word in keep_words:\n",
        "    assert(word in word2idx_small)\n",
        "\n",
        "  # map old idx to new idx\n",
        "  sentences_small = []\n",
        "  for sentence in indexed_sentences:\n",
        "    if len(sentence) > 1:\n",
        "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
        "      sentences_small.append(new_sentence)\n",
        "\n",
        "  return sentences_small, word2idx_small\n",
        "\n",
        "\n",
        "def get_bigram_probs(sentences,V,start_index,end_index,smoothing=1): #function to get bigram probability matrix\n",
        "  matrix = np.ones((V,V)) * smoothing # create matrix of V*V and intialize with smoothing parameter\n",
        "  for sentence in sentences:\n",
        "    for i in range (len(sentence)):\n",
        "      if(i==0):\n",
        "        matrix[start_index,sentence[i]]+=1\n",
        "      else:\n",
        "        matrix[sentence[i-1],sentence[i]]+=1\n",
        "      if(i==len(sentence)-1):\n",
        "        matrix[sentence[i],end_index]+=1\n",
        "  x = matrix.sum(axis=1,keepdims=True) #sum each row(to attain normalization)(P(B/A) = count(A followed by B)/count(A))\n",
        "  matrix/=x\n",
        "  return matrix          \n",
        "\n",
        "  V = len(word2idx)\n",
        "  print(\"Vocab size:\", V)\n",
        "\n",
        "  # we will also treat beginning of sentence and end of sentence as bigrams\n",
        "  # START -> first word\n",
        "  # last word -> END\n",
        "  start_idx = word2idx['START']\n",
        "  end_idx = word2idx['END']\n",
        "\n",
        "\n",
        "  # a matrix where:\n",
        "  # row = last word\n",
        "  # col = current word\n",
        "  # value at [row, col] = p(current word | last word)\n",
        "  bigram_probs = get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=0.1)\n",
        "\n",
        "\n",
        "  # train a shallow neural network model\n",
        "  D = 100\n",
        "  W1 = np.random.randn(V, D) / np.sqrt(V)\n",
        "  W2 = np.random.randn(D, V) / np.sqrt(D)\n",
        "\n",
        "  losses = []\n",
        "  epochs = 1\n",
        "  lr = 1e-2\n",
        "  \n",
        "  def softmax(a):\n",
        "    a = a - a.max()\n",
        "    exp_a = np.exp(a)\n",
        "    return exp_a / exp_a.sum(axis=1, keepdims=True)\n",
        "\n",
        "  # what is the loss if we set W = log(bigram_probs)?\n",
        "  W_bigram = np.log(bigram_probs)\n",
        "  bigram_losses = []\n",
        "\n",
        "  t0 = datetime.now()\n",
        "  for epoch in range(epochs):\n",
        "    # shuffle sentences at each epoch\n",
        "    random.shuffle(sentences)\n",
        "\n",
        "    j = 0 # keep track of iterations\n",
        "    for sentence in sentences:\n",
        "      # convert sentence into one-hot encoded inputs and targets\n",
        "      sentence = [start_idx] + sentence + [end_idx]\n",
        "      n = len(sentence)\n",
        "      inputs = np.zeros((n - 1, V))\n",
        "      targets = np.zeros((n - 1, V))\n",
        "      inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
        "      targets[np.arange(n - 1), sentence[1:]] = 1\n",
        "\n",
        "      # get output predictions\n",
        "      hidden = np.tanh(inputs.dot(W1))\n",
        "      predictions = softmax(hidden.dot(W2))\n",
        "\n",
        "      # do a gradient descent step\n",
        "      W2 = W2 - lr * hidden.T.dot(predictions - targets)\n",
        "      dhidden = (predictions - targets).dot(W2.T) * (1 - hidden * hidden)\n",
        "      W1 = W1 - lr * inputs.T.dot(dhidden)\n",
        "\n",
        "      # keep track of the loss\n",
        "      loss = -np.sum(targets * np.log(predictions)) / (n - 1)\n",
        "      losses.append(loss)\n",
        "\n",
        "      # keep track of the bigram loss\n",
        "      # only do it for the first epoch to avoid redundancy\n",
        "      if epoch == 0:\n",
        "        bigram_predictions = softmax(inputs.dot(W_bigram))\n",
        "        bigram_loss = -np.sum(targets * np.log(bigram_predictions)) / (n - 1)\n",
        "        bigram_losses.append(bigram_loss)\n",
        "\n",
        "\n",
        "      if j % 10 == 0:\n",
        "        print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
        "      j += 1\n",
        "\n",
        "plt.plot(losses)\n",
        "avg_bigram_loss = np.mean(bigram_losses)\n",
        "print(\"avg_bigram_loss:\", avg_bigram_loss)\n",
        "plt.axhline(y=avg_bigram_loss, color='r', linestyle='-') ## plotting horizontal line\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3wPraiHru2",
        "colab_type": "code",
        "outputId": "570fcc13-5ff6-4896-ac39-bdccd967acd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onYWkTAeH4bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git remote add origin https://github.com/sonakshisen1234/NLP_Course\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arEGKeNrIJZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuYS3eshISIM",
        "colab_type": "code",
        "outputId": "713f76f6-e76d-4316-a415-8694936139df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!git commit -m'bigram model with neural network'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@88740a255dd6.(none)')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-2j5sH3LHas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config  user.email \"sakshi.1331agarwal@gmail.com\"\n",
        "!git config  user.name \"sonakshisen1234\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnlFwvg7LpNM",
        "colab_type": "code",
        "outputId": "e62a1b8f-6a8d-407f-a691-6864b676cc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error: src refspec master does not match any.\n",
            "error: failed to push some refs to 'https://github.com/sonakshisen1234/NLP_Course'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}