{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigrams.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvRmpeO/T7mFVd0mz3z76O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonakshisen1234/NLP_Course/blob/master/bigrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOOa_OEgxpXi",
        "colab_type": "code",
        "outputId": "190b0c44-f71c-4d53-aac5-3e32698da746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import operator\n",
        "import numpy as np\n",
        "from future.utils import iteritems\n",
        "import random\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "KEEP_WORDS = set([\n",
        "  'king', 'man', 'queen', 'woman',\n",
        "  'italy', 'rome', 'france', 'paris',\n",
        "  'london', 'britain', 'england',\n",
        "])\n",
        "\n",
        "def get_sentences():\n",
        "  # returns 57340 of the Brown corpus\n",
        "  # each sentence is represented as a list of individual string tokens\n",
        "  return brown.sents()\n",
        "\n",
        "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
        "  sentences = get_sentences()\n",
        "  indexed_sentences = []\n",
        "\n",
        "  i = 2\n",
        "  word2idx = {'START': 0, 'END': 1}\n",
        "  idx2word = ['START', 'END']\n",
        "\n",
        "  word_idx_count = {\n",
        "    0: float('inf'),\n",
        "    1: float('inf'),\n",
        "  }\n",
        "\n",
        "  for sentence in sentences:\n",
        "    indexed_sentence = []\n",
        "    for token in sentence:\n",
        "      token = token.lower()\n",
        "      if token not in word2idx:\n",
        "        idx2word.append(token)\n",
        "        word2idx[token] = i\n",
        "        i += 1\n",
        "\n",
        "      # keep track of counts for later sorting\n",
        "      idx = word2idx[token]\n",
        "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
        "\n",
        "      indexed_sentence.append(idx)\n",
        "    indexed_sentences.append(indexed_sentence)\n",
        "\n",
        "\n",
        "\n",
        "  # restrict vocab size\n",
        "\n",
        "  # set all the words I want to keep to infinity\n",
        "  # so that they are included when I pick the most\n",
        "  # common words\n",
        "  for word in keep_words:\n",
        "    word_idx_count[word2idx[word]] = float('inf')\n",
        "\n",
        "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  word2idx_small = {}\n",
        "  new_idx = 0\n",
        "  idx_new_idx_map = {}\n",
        "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
        "    word = idx2word[idx]\n",
        "    #print(word, count)\n",
        "    word2idx_small[word] = new_idx\n",
        "    idx_new_idx_map[idx] = new_idx\n",
        "    new_idx += 1\n",
        "  # let 'unknown' be the last token\n",
        "  word2idx_small['UNKNOWN'] = new_idx \n",
        "  unknown = new_idx\n",
        "\n",
        "  assert('START' in word2idx_small)\n",
        "  assert('END' in word2idx_small)\n",
        "  for word in keep_words:\n",
        "    assert(word in word2idx_small)\n",
        "\n",
        "  # map old idx to new idx\n",
        "  sentences_small = []\n",
        "  for sentence in indexed_sentences:\n",
        "    if len(sentence) > 1:\n",
        "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
        "      sentences_small.append(new_sentence)\n",
        "\n",
        "  return sentences_small, word2idx_small\n",
        "\n",
        "def get_bigram_probs(sentences,V,start_index,end_index,smoothing=1): #function to get bigram probability matrix\n",
        "  matrix = np.ones((V,V)) * smoothing # create matrix of V*V and intialize with smoothing parameter\n",
        "  for sentence in sentences:\n",
        "    for i in range (len(sentence)):\n",
        "      if(i==0):\n",
        "        matrix[start_index,sentence[i]]+=1\n",
        "      else:\n",
        "        matrix[sentence[i-1],sentence[i]]+=1\n",
        "      if(i==len(sentence)-1):\n",
        "        matrix[sentence[i],end_index]+=1\n",
        "  x = matrix.sum(axis=1,keepdims=True) #sum each row(to attain normalization)(P(B/A) = count(A followed by B)/count(A))\n",
        "  matrix/=x\n",
        "  return matrix          \n",
        "\n",
        "def get_score(sentence): #we calculate the probability for each sentence but not normal probability(by multiplying various bigram probabilities)\n",
        "  score = 0              #instead we calculate logarithimic probability because normal probability will keep on getting smaller on multiplying(tend to zero)\n",
        "  for i in range (len(sentence)):\n",
        "    if(i==0):\n",
        "      score+=np.log(matrix[start_index,sentence[i]])\n",
        "    else:\n",
        "      score+=np.log(matrix[sentence[i-1],sentence[i]])\n",
        "    if(i==len(sentence)-1):\n",
        "      score+=np.log(matrix[sentence[i],end_index])\n",
        "  return score/(len(sentence) + 1) #(normalization) we divide by sentence length to prevent bias to shorter sentences(log proabilities are negative)  \n",
        "\n",
        "sentences, word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
        "V = len(word2idx)\n",
        "print(\"Vocab size:\", V)\n",
        "start_index = word2idx['START']\n",
        "end_index = word2idx['END'] \n",
        "matrix = get_bigram_probs(sentences, V, start_index, end_index, smoothing=0.1) \n",
        "# a function to map word indexes back to real words\n",
        "idx2word = dict((v, k) for k, v in iteritems(word2idx))\n",
        "def get_words(sentence):\n",
        "    return ' '.join(idx2word[i] for i in sentence)  \n",
        "\n",
        "\n",
        "# when we sample a fake sentence, we want to ensure not to sample\n",
        "# start token or end token  \n",
        "sample_probs = np.ones(V)  \n",
        "sample_probs[start_index] = 0  \n",
        "sample_probs[end_index] = 0  \n",
        "sample_probs /= sample_probs.sum()    \n",
        "\n",
        "real_idx = np.random.choice(len(sentences))\n",
        "real = sentences[real_idx]    \n",
        "print(real)\n",
        "# fake sentence    \n",
        "fake = np.random.choice(V, size=len(real), p=sample_probs)    \n",
        "print(fake)\n",
        "\n",
        "print(\"REAL:\", get_words(real), \"SCORE:\", get_score(real))    \n",
        "print(\"FAKE:\", get_words(fake), \"SCORE:\", get_score(fake))    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "Vocab size: 10001\n",
            "[28, 22, 41, 47, 10000, 18, 193, 21, 13, 134, 228, 64, 133, 45, 71, 888, 439, 103, 91, 1251, 16, 228, 15]\n",
            "[8925 3168 3489 8459 8223 1308 6830 1775  809  715 9765 1478 3042 1698\n",
            " 5566  603 8668 5862 9236 8282 5248 5158 4837]\n",
            "REAL: it is not an UNKNOWN to say that the state government has little or no fiscal control over these units of government . SCORE: -4.565574228697625\n",
            "FAKE: loneliness producing tube assessing limitation broad ironic location month below compost store moves passing lest move tearing notably fbi twenties halfway rugged dominated SCORE: -9.319580136357972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ag4IHPfZCT",
        "colab_type": "code",
        "outputId": "fc202be9-e89f-47b1-c058-8ec895a5c165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWTjjQi3f4Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config user.email \"sakshi.1331agarwal@gmail.com\"\n",
        "!git config user.name \"sonakshisen1234\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPvQMUtGgPxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add -A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1k2RxVggYQK",
        "colab_type": "code",
        "outputId": "8b0b7c82-5118-4892-87d9-69bf0d5e18f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!git commit -m \"first commit\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master (root-commit) cdd89ab] first commit\n",
            " 20 files changed, 50775 insertions(+)\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/.metricsUUID\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/logs/2020.06.10/16.27.11.187541.log\n",
            " create mode 100644 .config/logs/2020.06.10/16.27.29.206693.log\n",
            " create mode 100644 .config/logs/2020.06.10/16.27.42.054200.log\n",
            " create mode 100644 .config/logs/2020.06.10/16.27.46.450102.log\n",
            " create mode 100644 .config/logs/2020.06.10/16.28.00.359521.log\n",
            " create mode 100644 .config/logs/2020.06.10/16.28.00.889996.log\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFgq5jOJgvD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git remote add origin https://sonakshisen1234:gitgithub123@github.com/sonakshisen1234/NLP_Course.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSDdzIoviYj4",
        "colab_type": "code",
        "outputId": "d0f968b9-dba5-4ab0-a0ab-19bb71d9c57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 27, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (19/19), done.\n",
            "Writing objects: 100% (27/27), 8.42 MiB | 2.22 MiB/s, done.\n",
            "Total 27 (delta 4), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (4/4), done.\u001b[K\n",
            "To https://github.com/sonakshisen1234/NLP_Course.git\n",
            " * [new branch]      master -> master\n",
            "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}